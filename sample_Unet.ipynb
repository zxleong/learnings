{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from math import exp\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Conv2d: 1-1                            [-1, 2, 300, 300]         20\n",
      "├─Conv2d: 1-2                            [-1, 4, 300, 300]         76\n",
      "├─MaxPool2d: 1-3                         [-1, 4, 150, 150]         --\n",
      "├─Conv2d: 1-4                            [-1, 8, 150, 150]         296\n",
      "├─Conv2d: 1-5                            [-1, 8, 150, 150]         584\n",
      "├─MaxPool2d: 1-6                         [-1, 8, 75, 75]           --\n",
      "├─Conv2d: 1-7                            [-1, 16, 75, 75]          1,168\n",
      "├─Conv2d: 1-8                            [-1, 16, 75, 75]          2,320\n",
      "├─MaxPool2d: 1-9                         [-1, 16, 15, 15]          --\n",
      "├─Conv2d: 1-10                           [-1, 32, 15, 15]          4,640\n",
      "├─Conv2d: 1-11                           [-1, 32, 15, 15]          9,248\n",
      "├─MaxPool2d: 1-12                        [-1, 32, 3, 3]            --\n",
      "├─Conv2d: 1-13                           [-1, 64, 3, 3]            18,496\n",
      "├─Conv2d: 1-14                           [-1, 64, 3, 3]            36,928\n",
      "├─MaxPool2d: 1-15                        [-1, 64, 1, 1]            --\n",
      "├─ConvTranspose2d: 1-16                  [-1, 32, 3, 3]            18,464\n",
      "├─Conv2d: 1-17                           [-1, 32, 3, 3]            18,464\n",
      "├─ConvTranspose2d: 1-18                  [-1, 16, 15, 15]          4,624\n",
      "├─Conv2d: 1-19                           [-1, 16, 15, 15]          4,624\n",
      "├─ConvTranspose2d: 1-20                  [-1, 8, 75, 75]           2,056\n",
      "├─Conv2d: 1-21                           [-1, 8, 75, 75]           1,160\n",
      "├─ConvTranspose2d: 1-22                  [-1, 4, 150, 150]         516\n",
      "├─Conv2d: 1-23                           [-1, 4, 150, 150]         292\n",
      "├─ConvTranspose2d: 1-24                  [-1, 1, 300, 300]         65\n",
      "==========================================================================================\n",
      "Total params: 124,041\n",
      "Trainable params: 124,041\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 94.75\n",
      "==========================================================================================\n",
      "Input size (MB): 0.34\n",
      "Forward/backward pass size (MB): 11.16\n",
      "Params size (MB): 0.47\n",
      "Estimated Total Size (MB): 11.98\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, neurons=2, kern_sz=3, in_channels=1, out_channels=1):\n",
    "        super(Generator,self).__init__() \n",
    "        #encoder block \n",
    "        self.conv1a = nn.Conv2d(in_channels=in_channels, out_channels=neurons, kernel_size=kern_sz, stride=1, padding=1)\n",
    "        self.conv1b = nn.Conv2d(in_channels=neurons, out_channels=neurons*2, kernel_size=kern_sz, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(neurons*2)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size = 3, stride=2, padding=1)\n",
    "\n",
    "        self.conv2a = nn.Conv2d(in_channels=neurons*2, out_channels=neurons*4, kernel_size=kern_sz, stride=1, padding=1)\n",
    "        self.conv2b = nn.Conv2d(in_channels=neurons*4, out_channels=neurons*4, kernel_size=kern_sz, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(neurons*4)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = 3, stride=2,padding=1) \n",
    "\n",
    "        self.conv3a = nn.Conv2d(in_channels=neurons*4, out_channels=neurons*8, kernel_size=kern_sz, stride=1, padding=1)\n",
    "        self.conv3b = nn.Conv2d(in_channels=neurons*8, out_channels=neurons*8, kernel_size=kern_sz, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(neurons*8)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size = 3, stride=5,padding=1) \n",
    "\n",
    "        self.conv4a = nn.Conv2d(in_channels=neurons*8, out_channels=neurons*16, kernel_size=kern_sz, stride=1, padding=1)\n",
    "        self.conv4b = nn.Conv2d(in_channels=neurons*16, out_channels=neurons*16, kernel_size=kern_sz, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(neurons*16)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size = 3, stride=5,padding=1) \n",
    "    \n",
    "        # Bottleneck\n",
    "        self.conv5a = nn.Conv2d(in_channels=neurons*16, out_channels=neurons*32, kernel_size=kern_sz, stride=1, padding=1)\n",
    "        self.conv5b = nn.Conv2d(in_channels=neurons*32, out_channels=neurons*32, kernel_size=kern_sz, stride=1, padding=1) \n",
    "        self.bn5 = nn.BatchNorm2d(neurons*32)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size = 3, stride=3,padding=1) \n",
    "        \n",
    "        # Decoder block (Upsampling)\n",
    "        self.upconv4 = nn.ConvTranspose2d(in_channels=neurons*32, out_channels=neurons*16, kernel_size=3, stride=3,padding=0)\n",
    "        self.dec4 = nn.Conv2d(in_channels=neurons*32, out_channels=neurons*16, kernel_size=kern_sz, stride=1, padding=1) \n",
    "        self.bnc4 = nn.BatchNorm2d(neurons*16)\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(in_channels=neurons*16, out_channels=neurons*8, kernel_size=3, stride=5,padding=0,output_padding=2)\n",
    "        self.dec3 = nn.Conv2d(in_channels=neurons*16, out_channels=neurons*8, kernel_size=kern_sz, stride=1, padding=1) \n",
    "        self.bnc3 = nn.BatchNorm2d(neurons*8)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(in_channels=neurons*8, out_channels=neurons*4, kernel_size=4, stride=5,padding=0,output_padding=1)\n",
    "        self.dec2 = nn.Conv2d(in_channels=neurons*8, out_channels=neurons*4, kernel_size=kern_sz, stride=1, padding=1) \n",
    "        self.bnc2 = nn.BatchNorm2d(neurons*4)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(in_channels=neurons*4, out_channels=neurons*2, kernel_size=4, stride=2,padding=1)\n",
    "        self.dec1 = nn.Conv2d(in_channels=neurons*4, out_channels=neurons*2, kernel_size=kern_sz, stride=1, padding=1)\n",
    "        self.bnc1 = nn.BatchNorm2d(neurons*2)\n",
    "\n",
    "        self.fupconv = nn.ConvTranspose2d(in_channels=neurons*2, out_channels=out_channels, kernel_size=4, stride=2,padding=1)\n",
    "        \n",
    "    # def forward(self, x):\n",
    "    #     # Encoder\n",
    "    #     x1 = self.conv1b(self.conv1a(x))\n",
    "    #     x1 = self.bn1(self.pool1(x1)) \n",
    "    #     x1 = F.relu(x1)\n",
    "    #     x2 = self.conv2b(self.conv2a(x1))\n",
    "    #     x2 = self.bn2(self.pool2(x2)) \n",
    "    #     x2 = F.relu(x2)\n",
    "    #     x3 = self.conv3b(self.conv3a(x2))\n",
    "    #     x3 = self.bn3(self.pool3(x3)) \n",
    "    #     x3 = F.relu(x3)\n",
    "    #     x4 = self.conv4b(self.conv4a(x3))\n",
    "    #     x4 = self.bn4(self.pool4(x4)) \n",
    "    #     x4 = F.relu(x4)\n",
    "\n",
    "    #     # #bottleneck\n",
    "    #     x5 = F.relu(self.bn5(self.pool5(self.conv5b(self.conv5a(x4)))))\n",
    "\n",
    "    #     # # Decoder\n",
    "    #     x = torch.cat([self.upconv4(x5),x4],dim=1)\n",
    "    #     x = self.bnc4(self.dec4(x)) \n",
    "    #     x = F.relu(x)\n",
    "    #     x = torch.cat([self.upconv3(x),x3],dim=1)\n",
    "    #     x = self.bnc3(self.dec3(x)) \n",
    "    #     x = F.relu(x)\n",
    "    #     x = torch.cat([self.upconv2(x),x2],dim=1)\n",
    "    #     x = self.bnc2(self.dec2(x)) \n",
    "    #     x = F.relu(x)\n",
    "    #     x = torch.cat([self.upconv1(x),x1],dim=1)\n",
    "    #     x = self.bnc1(self.dec1(x))\n",
    "    #     x = F.relu(x)\n",
    "    #     x = F.sigmoid(self.fupconv(x))\n",
    "        \n",
    "    #     return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.conv1b(self.conv1a(x))\n",
    "        x1 = self.pool1(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x2 = self.conv2b(self.conv2a(x1))\n",
    "        x2 = self.pool2(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        x3 = self.conv3b(self.conv3a(x2))\n",
    "        x3 = self.pool3(x3)\n",
    "        x3 = F.relu(x3)\n",
    "        x4 = self.conv4b(self.conv4a(x3))\n",
    "        x4 = self.pool4(x4)\n",
    "        x4 = F.relu(x4)\n",
    "\n",
    "        # #bottleneck\n",
    "        x5 = F.relu(self.pool5(self.conv5b(self.conv5a(x4))))\n",
    "\n",
    "        # # Decoder\n",
    "        x = torch.cat([self.upconv4(x5),x4],dim=1)\n",
    "        x = self.dec4(x) \n",
    "        x = F.relu(x)\n",
    "        x = torch.cat([self.upconv3(x),x3],dim=1)\n",
    "        x = self.dec3(x)\n",
    "        x = F.relu(x)\n",
    "        x = torch.cat([self.upconv2(x),x2],dim=1)\n",
    "        x = self.dec2(x)\n",
    "        x = F.relu(x)\n",
    "        x = torch.cat([self.upconv1(x),x1],dim=1)\n",
    "        x = self.dec1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.sigmoid(self.fupconv(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "model = Generator(neurons=2, kern_sz=3, in_channels=1, out_channels=1)\n",
    "\n",
    "summary(model, (1, 300,300));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "class DatasetLoad(Dataset):\n",
    "    def __init__(self, X_train, y_train ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features (numpy.ndarray): Numpy array containing the features.\n",
    "            labels (numpy.ndarray): Numpy array containing the labels.\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X_train = torch.from_numpy(self.X_train[idx]).float()\n",
    "        y_train = torch.from_numpy(self.y_train[idx]).float()\n",
    "\n",
    "        return X_train.to(device), y_train.to(device)\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### input and output dataset here\n",
    "\n",
    "X_train = np.random.random((1000,1,300,300))\n",
    "y_train = np.random.random((1000,1,300,300))\n",
    "X_val = np.random.random((25,1,300,300))\n",
    "y_val = np.random.random((25,1,300,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 1, 300, 300])\n",
      "torch.Size([25, 1, 300, 300])\n"
     ]
    }
   ],
   "source": [
    "X_val_torch = torch.tensor(X_val).float().to(device)\n",
    "y_val_torch = torch.tensor(y_val).float().to(device)\n",
    "print(X_val_torch.shape)\n",
    "print(y_val_torch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DatasetLoad(X_train,y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Generator(neurons=2, kern_sz=3, in_channels=1, out_channels=1).to(device)\n",
    "lr = 0.0001\n",
    "loss_fn_mse = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "num_epochs = 250\n",
    "BS = 20\n",
    "\n",
    "total_samples = len(X_train)\n",
    "n_batches = int(total_samples/ BS)\n",
    "print_batches = (total_samples / BS ) / 4\n",
    "\n",
    "record_NNLoss = []\n",
    "record_ssim=[]\n",
    "val_record_NNLoss = []\n",
    "val_record_ssim=[np.nan]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training Loop!\n",
      "Epoch 0 \n",
      "[0/250], Batch:[0/50]\t Loss: 0.083301976 \t\n",
      "[0/250], Batch:[25/50]\t Loss: 0.083440006 \t\n",
      "[0/250], Batch:[50/50]\t Loss: 0.083375834 \t\n",
      "[0/250], Batch:[75/50]\t Loss: 0.083442397 \t\n",
      "EPOCH 0 Training -> Loss: 0.083396536 \t \n",
      "EPOCH 0 Validation -> Loss: 0.083376840 \t\n",
      "Epoch 1 \n",
      "[1/250], Batch:[0/50]\t Loss: 0.083449826 \t\n",
      "[1/250], Batch:[25/50]\t Loss: 0.083297335 \t\n",
      "[1/250], Batch:[50/50]\t Loss: 0.083247833 \t\n",
      "[1/250], Batch:[75/50]\t Loss: 0.083489195 \t\n",
      "EPOCH 1 Training -> Loss: 0.083357497 \t \n",
      "EPOCH 1 Validation -> Loss: 0.083349720 \t\n",
      "Epoch 2 \n",
      "[2/250], Batch:[0/50]\t Loss: 0.083445534 \t\n",
      "[2/250], Batch:[25/50]\t Loss: 0.083279826 \t\n",
      "[2/250], Batch:[50/50]\t Loss: 0.083389431 \t\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'''\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'''\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch))\n\u001b[0;32m     12\u001b[0m batch_Loss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 13\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#zero the parameter gradients\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#activate model training\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\Zack\\miniconda3\\envs\\MLenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Zack\\miniconda3\\envs\\MLenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Zack\\miniconda3\\envs\\MLenv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Zack\\miniconda3\\envs\\MLenv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[32], line 18\u001b[0m, in \u001b[0;36mDatasetLoad.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     15\u001b[0m X_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train[idx])\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     16\u001b[0m y_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train[idx])\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_train\u001b[38;5;241m.\u001b[39mto(device), \u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Reproducibility\n",
    "torch.manual_seed(155)\n",
    "np.random.seed(155)\n",
    "\n",
    "traininglog=dict()\n",
    "e_record_Loss = []\n",
    "val_store_Loss = []\n",
    "print('''Start Training Loop!''')\n",
    "for epoch in range(num_epochs):\n",
    "    print('''Epoch {} '''.format(epoch))\n",
    "    \n",
    "    batch_Loss = []\n",
    "    for batch_i, (X_batch,y_batch) in enumerate(train_dataloader):\n",
    "\n",
    "        #zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #activate model training\n",
    "        model.train()\n",
    "        #predict\n",
    "        prediction = model(X_batch)\n",
    "        #calculate loss \n",
    "        Loss = loss_fn_mse(prediction,y_batch) \n",
    "        \n",
    "        #compute metric\n",
    "        ### add your own metric here\n",
    "                \n",
    "        #Backpropagate\n",
    "        Loss.backward()\n",
    "        #Apply optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Print out batch progress\n",
    "        if batch_i % print_batches == 0:\n",
    "            print('[%d/%d], Batch:[%d/%d]\\t Loss: %.9f \\t'\n",
    "                  % (epoch,num_epochs,batch_i,n_batches,Loss.item()))\n",
    "        #Save batch losses\n",
    "        batch_Loss.append(Loss.item())\n",
    "        \n",
    "    #Compute epoch losses\n",
    "    epoch_Loss = np.mean(batch_Loss)\n",
    "    \n",
    "    #Print out Epoch Progress\n",
    "    print('EPOCH %d Training -> Loss: %.9f \\t ' \n",
    "           %(epoch,epoch_Loss))\n",
    "\n",
    "    #save epoch progress\n",
    "    e_record_Loss.append(epoch_Loss)\n",
    "    \n",
    "    #save into dictionary\n",
    "    traininglog['Epoch_Loss'] = e_record_Loss\n",
    "    \n",
    "    ###############################\n",
    "    ######### Validation ##########\n",
    "    ###############################\n",
    "    with torch.no_grad():\n",
    "\n",
    "        #activate model evaluation\n",
    "        model.eval()\n",
    "        #predict\n",
    "        val_prediction = model(X_val_torch)\n",
    "        #calculate loss\n",
    "        val_Loss = loss_fn_mse(val_prediction,y_val_torch) \n",
    "\n",
    "\n",
    "        #Print validation progress\n",
    "        print('EPOCH %d Validation -> Loss: %.9f \\t'\n",
    "               %(epoch, val_Loss.item()))\n",
    "    \n",
    "\n",
    "    #save into dictionary\n",
    "    val_store_Loss.append(val_Loss.item())\n",
    "    traininglog['val_Loss'] = val_store_Loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
